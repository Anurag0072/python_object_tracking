import csvimport nltkimport numpy as npimport tensorflow as tffrom tensorflow.python.keras.layers import Embedding, Dropout, Bidirectional, Dense, LSTMfrom tensorflow.python.keras.models import Sequentialfrom tensorflow.python.keras.preprocessing.sequence import pad_sequencesfrom tensorflow.python.keras.preprocessing.text import Tokenizernltk.download('stopwords')from nltk.corpus import stopwordsSTOPWORDS = set(stopwords.words('english'))#We set the hyper-Parameters that are required to build and train the modelvocab_size = 5000 # make the top list of words (common words)embedding_dim = 64max_length = 200trunc_type = 'post'padding_type = 'post'oov_tok = '<OOV>' # OOV = Out of Vocabularytraining_portion = .8#We populate the list of articles and labels from the data and also remove the stopwords.articles = []labels = []with open(r"F:\Datasets\bbc-text.csv",'r') as csvfile:    reader = csv.reader(csvfile, delimiter=',')    next(reader)    for row in reader:        labels.append(row[0])        article = row[1]        for word in STOPWORDS:            token = ' ' + word + ' '            article = article.replace(token, ' ')            article = article.replace(' ', ' ')        articles.append(article)        print(len(labels))        print(len(articles))#Create Training and Validation Set    train_size = int(len(articles) * training_portion)    train_articles = articles[0: train_size]    train_labels = labels[0: train_size]    validation_articles = articles[train_size:]    validation_labels = labels[train_size:]    print("train_size",train_size)    print(f"train_article{len(train_articles)}")    print("train_lables",len(train_labels))    print("validation_articles",len(validation_articles))    print("validation_lables",len(validation_labels))#Tokenization    tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)    tokenizer.fit_on_texts(train_articles)    word_index = tokenizer.word_index#Convert to Sequences    train_sequences = tokenizer.texts_to_sequences(train_articles)#Sequence Truncation and Padding    train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)    tokenizer.fit_on_texts(train_articles)    word_index = tokenizer.word_index    train_sequences = tokenizer.texts_to_sequences(train_articles)    train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)    validation_sequences = tokenizer.texts_to_sequences(validation_articles)    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)label_tokenizer = Tokenizer()label_tokenizer.fit_on_texts(labels)training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))model =Sequential()#Embedding Layermodel.add(Embedding(vocab_size, embedding_dim))model.add(Dropout(0.5))model.add(Bidirectional(LSTM(embedding_dim)))model.add(Dense(6, activation='softmax'))model.summary()#Compile the Modelopt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)model.compile(    loss='sparse_categorical_crossentropy',    optimizer=opt,    metrics=['accuracy'],)#Train the Model#num_epochs = 1#history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)#Prediction#txt = [ say:  he will announce an election when he wants to announce an election.  the move will signal a frantic week at westminster as the government is likely to try to get key legislation through parliament. the government needs its finance bill  covering the budget plans  to be passed before the commons closes for business at the end of the session on 7 april.  but it will also seek to push through its serious and organised crime bill and id cards bill. mr marr said on wednesday s today programme:  there s almost nobody at a senior level inside the government or in parliament itself who doesn t expect the election to be called on 4 or 5 april.  as soon as the commons is back after the short easter recess  tony blair whips up to the palace  asks the queen to dissolve parliament ... and we re going.  the labour government officially has until june 2006 to hold general election  but in recent years governments have favoured four-year terms."]txt=["The party has organised a two-day national convention of its OBC morcha in Bihar's Patna which will brainstorm the strategy to reach out to the OBC vote bank across the country."]seq = tokenizer.texts_to_sequences(txt)padded = pad_sequences(seq, maxlen=max_length)pred = model.predict(padded)labels = ['sport', 'bussiness', 'politics', 'tech', 'entertainment']print(pred)print(np.argmax(pred))print(labels[np.argmax(pred)-1])